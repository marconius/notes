====== Introduction ======

INF4230 - Intelligence Artificielle. Ce cours est donnÃ© Ã  UQÃ€M. Il est un cours optionnel dans le cadre du Bac en informatique et gÃ©nie logiciel.

====== Raisonnement Probabiliste ======

== Survol ==

  * Le raisonnement probabiliste est utile quand l'environnement est [[#environnement|non-deterministe]].
  * ...

====== RÃ©seaux BayÃ©siens ======

=== Survol ===

Un rÃ©seau BayÃ©sien est une graphe jumelÃ©e avec de l'information probabiliste. Une rÃ©vision de la [[inf4680|thÃ©orie des probabilitÃ©s]] et prÃ©alable.
 
  * Les nÅ“uds sont les variables.
  * Les arcs sont les **dÃ©pendances** entre les variables et sont orientÃ©s en direction des causes aux effets ou vice-versa.
  * Chaque variable est accompagnÃ©e d'une table de probabilitÃ© ou **table de probabilitÃ©s conditionnelles (TPC)**.
  * Les TPC reprÃ©sentent des morceaux de la distribution conjointe des probabilitÃ©s. 

== IndÃ©pendance et indÃ©pendance conditionnel entre les variables ==

Dans le cas gÃ©nÃ©rale, on peut savoir si une variable A est indÃ©pendante de B si la connaissance de B n'a aucun effet sur la probabilitÃ© de A et vice-versa. Formellement: A est indÃ©pendant de B si P(A|B) = P(A) //ou// P(B|A) = P(B) //ou// P(A,B) = P(A)P(B).

Dans un rÃ©seaux BayÃ©sien, considÃ¨re l'indÃ©pendance conditionnelle. Donc si le noeud de la variable B est un non-**descendant** de A, A est indÃ©pendante de B si on connaÃ®t les parents de A. Si B est n'importe qu'elle autre type de noeud, A est indÃ©pendante de B sachant les parents, enfants et parents des enfants de A (c-Ã -d sa **couverture de Markov**).

== InfÃ©rence exacte ==

InfÃ©rence c'est Â«l'interogationÂ» du RB pour trouver la probabilitÃ© d'une requÃªte.

L'**infÃ©rence par Ã©numÃ©ration** est basÃ© sur l'Ã©quivalences P(Xâˆ£e) = ğ›¼ P(X,e) = ğ›¼ âˆ‘y, P(X,e,y) (voir distribution conjointe des probabilitÃ©s) qui sont appliquÃ©s aux TPC d'un rÃ©seau bayÃ©sien ainsi: P(Xâˆ£e) = ğ›¼ âˆ‘y, P(X,e,h1,...,hn) = ğ›¼ âˆ‘h1,...âˆ‘hn, P(Xâˆ£parents de X)P(eâˆ£parents de e)P(h1âˆ£parents de h1)...P(hnâˆ£parents de hn) oÃ¹ X est la variable pour laquelle on fait la requÃªte, e1 ... en sont les Ã©vidences, et h1 ... hn sont les variables cachÃ©es de notre rÃ©seaux. 

Le point faible de l'infÃ©rence exacte est la complexitÃ© temporelle Ã©levÃ©e des diffÃ©rents algorithmes. Pour cette raison on a l' ...

== InfÃ©rence approximative ==

...

=== DÃ©tails ===

== IndÃ©pendance conditionnelle ==

...
===== RÃ©seaux Bayesiens dynamiques =====

...

====== Robtique ======

...

====== Processus dÃ©cisionnel de Markov (MDP) ======

...

====== Apprentissage Machine ======

...

===== L'apprentissage =====

...

===== RÃ©seaux Neuronales artificiels (RNA) =====

...

==== f(net) ====

...

==== Apprentissage compÃ©titif ====

...

====== Robotique ======

...

===== Probabilistic Road Map =====

...

===== Rapid random Trees =====

...

====== RÃ©ponses aux exercices ======

Des rÃ©ponses originales a un variÃ©tÃ©s d'exercises. Attention Ã  ne pas transgresser les droits d'auteurs et les politiques regardant le plagiat.

===== Beaudry, Eric; Â«Exemples de questions d'examen final (Hiver 2013)Â» =====

== Q1 ==

**a)** T et D sont indÃ©pendant car P(t|D) = P(t) = 0.8

**b)** E est dÃ©pendante de C parce qu'on ne connaÃ®t pas la valeur de D est donc on ne connaÃ®t pas la valeur de tous ces parents.

**c)** E est indÃ©pendante de C dans les cas oÃ¹ D est vraie //ou// D est fausse car les variables dans sa couverture de Markov sont tous connus.

**d)** P(Eâˆ£Â¬c) = ğ›¼ âˆ‘tâˆ‘dâˆ‘r, P(T)P(Dâˆ£Â¬c,R)P(R)P(eâˆ£T,D)P(Â¬c)

P(eâˆ£Â¬c) =

^ T ^ D ^ R ^ P(T) ^ P(Dâˆ£Â¬c,R) ^ P(R) ^ P(eâˆ£T,D) ^ P(Â¬c) ^ âˆ      ^
| V | V | V | 0,8  | 0,6       | 0,8  |  ...     |  ...  | ...    |
| F | V | V | 0,2  | 0,6       | 0,8  |  ...     |  ...  | ...    |
| V | F | V | 0,8  | 0,6       | 0,8  |  ...     |  ...  | ...    |
| F | F | V | 0,2  | 0,6       | 0,8  |  ...     |  ...  | ...    |
| V | V | F | 0,8  | 0,4       | 0,2  |  ...     |  ...  | ...    |
| F | V | F | 0,2  | 0,4       | 0,2  |  ...     |  ...  | ...    |
| V | F | F | 0,8  | 0,4       | 0,2  |  ...     |  ...  | ...    |
| F | F | F | 0,2  | 0,4       | 0,2  |  ...     |  ...  | ...    |
|                                             âˆ‘ = |||||||| 0,38192 |

Pour obtenir ğ›¼ on fait la mÃªme chose pour P(Â¬eâˆ£Â¬c),ce qui donne 0,27348. Donc ğ›¼ = 1 / (0,38192 + 0,27348) = 1,5257857797. Donc P(eâˆ£Â¬c) = 1,5257857797 * 1,12 = __0,582728105__.

et P(Eâˆ£Â¬c,Â¬t) = ğ›¼ âˆ‘dâˆ‘r, P(Dâˆ£Â¬c,R)P(R) ...

**d)** ...

== Q2 ==

**a)**

{{:q3-a.png?|}}

**b)**

P(X1=Aâˆ£X0=<1/3; 1/3; 1/3>) = ğ›¼ âˆ‘x0 P( ... 

== Q6 ==

**a)** L'environnement du problÃ¨me est totalement observable, dÃ©terministe, (Ã©pisodique ou sÃ©quentielle), statique, discret (mÃªme si dans une vraie intersection le problÃ¨me est continue, la rÃ©alitÃ© est discrÃ©tisÃ©e dans la simulation) et il n'y a qu'un agent (l'itersection intelligente qui contrÃ´le les autos).

**b)** 
